---
layout: post
title: "How Web Scraping eases my job searching pain? <br /> - Part I : Scrape contents from one URL"
author: Yin-Ting 
date: 2017-06-05
photocredit: Yin-Ting Chou
categories: [Python]
tags: [Web Scraping]
cover: "/assets/post_image/IMG_3401.JPG"
---
### Overview
The process of finding jobs online can definitely be a torture. Every day is started by sitting in front of computer, browsing different kind of job searching websites and trying to track every jobs I have read and then categorizing them into interested or not interested. The whole process is mostly about clicking on different pages and websites, copying and pasting words. At first, you may have enough patience to read and track about 50 or more jobs a day, but when time gets long, you start to feel depressed and feel like yourself is just like a robot. If you have this same kind of experience, you are welcom to check out this post and see how I use automated web scraping techniques in [Python 3.6.0](https://www.python.org/downloads/release/python-360/) to make my job searching process easier, more efficient and much more fun! <br />
<br />
<img src="{{ site.baseurl }}/assets/image/web_jobs.png" style="width:100%"/>
This is just a quick look for one of my final output which was automatically generated by running the Python code. The good thing for this sheet is that I can have a big overview for all the job searching results with their required skills, education level, major and self-interested key words. So, I can easily prioritize the jobs and only focus on ones I think I can be a good fit. Compared to clicking all the job link one by one on all the job searching websites, it really not only save me a lot of time for the first screening but also make me healthier in mental which to me is the most important thing! If you also think this sheet can be a good idea for your job searching process, please keep reading and grab anything worthy to you. 

If you are not a Python 3.6.0 user or only know little about it, don't worry! Before I did this project, I also know nothing about Python and I just learned by doing. You can find all the example code in my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example). My code may not be the most perfect but it works well on me. 

*** 

### Details
* **<font size="4">References</font>** <br />
  Thank you all so much! 
  * [JianhuaHuang - Data_Scientist_Skills_Python_R](https://github.com/JianhuaHuang/Data_Scientist_Skills_Python_R)
  * [Greg Reda - Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/)
  * [Sung Pil Moon - Web Scraping company data from Indeed.com and Dice.com](https://blog.nycdatascience.com/student-works/project-3-web-scraping-company-data-from-indeed-com-and-dice-com/)
  * [Diego De Lazzari - Landing my dream job by scraping Glassdoor.com](https://blog.nycdatascience.com/student-works/web-scraping/glassdoor-web-scraping/)
  * [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
  
* **<font size="4">Goals</font>** <br />
  <ol>
    <li style="font-weight: bold"> Automatically generate a sheet that can include today's all searching results from 4 different job searching websites (<a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a>, <a href="http://www.dice.com/">Dice</a>, <a href="http://www.careerbuilder.com/">Careerbuilder</a>).
    <li style="font-weight: bold">For each job result, I would like to know the following things: <br />
      <ul style="font-weight: normal">
        <li>From which Job Searching Website </li>
        <li>Job Title </li>
        <li>Job Company </li>
        <li>Location </li>
        <li>Job ID (related to Job Link) </li>
        <li>Job Link </li>
        <li>Job Type </li>
        <li>Required Skills </li>
        <li>Required Education Level </li>
        <li>Preferred Major </li>
        <li>Interesting Keywords </li></ul>

* **<font size="4">Steps</font>** <br />
    For all the following steps, I will use web scraping [Careerbuilder](http://www.careerbuilder.com/) as an example. If you are interested in how to do it on other job searching websites ([Indeed](https://www.indeed.com/), [Monster](https://www.monster.com/) and [Dice](http://www.dice.com/)), please check out my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example). <br />
  Example searching criteria: 
    - job searching website: *Careerbuilder* 
    - searching keywords: *Data Scientist* 
    - city: *New York* 
    - state: *NY* 
    
    1. **Know the URL of the webpage you are going to scrape and know how they related to your searching criteria.** <br />
    The URL for our example, [http://www.careerbuilder.com/jobs-data-scientist-in-New-York,NY](http://www.careerbuilder.com/jobs-data-scientist-in-New-York,NY). 
    
        ``` python
        ########################################################
        #################### IMPORT LIBRARY ####################
        ########################################################
        import bs4
        import numpy
        import pandas
        import re
        import requests
        
        ###################################################
        #################### ARGUMENTS ####################
        ###################################################
        input_job = "data scientist"
        input_quote = False # add "" on your searching keywords
        input_city = "New York"
        input_state = "NY"
        sign = "-"
        BASE_URL_careerbuilder = 'http://www.careerbuilder.com'

        #####################################################
        ##### Function for Transform searching keywords #####
        #####################################################
        # The default "quote = False"
        def transform(input,sign, quote = False):
          syntax = input.replace(" ", sign)
          if quote == True:
            syntax = ''.join(['%2522', syntax, '%2522'])
          return(syntax)

        ######################################
        ########## Generate the URL ##########
        ######################################
        url_careerbuilder_list = [ BASE_URL_careerbuilder, '/jobs-',
            transform(input_job, sign, input_quote), '-in-',
            transform(input_city, sign, input_quote),',', input_state]
        url_careerbuilder = ''.join(url_careerbuilder_list)
        print(url_careerbuilder)
        ```
        `http://www.careerbuilder.com/jobs-data-scientist-in-New-York,NY`
    
  
    2. **Scrape general contents from the webpage.** <br />
      To catch the important info from the webpage, we have to start from its HTML code. For the chrome, click right mouse button and choose *Inspect*. And for the Safari, do the same thing but choose *Inspect Element*. Then now, we can see all the HTML code for the webpage. 
        * **Total Number of the results** 
          <img src="{{ site.baseurl }}/assets/image/web_html.png" style="width:100%">
          ``` python
          # get the HTML code from the URL
          rawcode_careerbuilder = requests.get(url_careerbuilder)
          
          # Choose "lxml" as parser 
          soup_careerbuilder = bs4.BeautifulSoup(rawcode_careerbuilder.text, "lxml")
          
          # total number of results
          num_total_careerbuilder = soup_careerbuilder.find('div', {'class' : 'count'}).contents[0]
          print(num_total_careerbuilder)
          num_total_careerbuilder = int(re.sub('[\(\)\{\}<>]', '', num_total_careerbuilder).split()[0])
          print(num_total_careerbuilder)
          ```
          <pre><code class="language-text" data-lang="text">(29 Jobs)
          29</code></pre>
          
        * **Total Number of pages** <br />
          By doing some test and observation, I know that there are 25 positions in one page. Then, I can calculate the total number of pages. 
          ``` python
          # total number of pages
          num_pages_careerbuilder = int(numpy.ceil(num_total_careerbuilder/25.0))
          print(num_pages_careerbuilder)
          ```
          `2`
    
    3. **Scrape all positions with their basic info for each page.** <br />
        * **The basic info for each position is in its `<div class="job-row">...</div>` chunk.** So, in our code, the first steps would be picking out all the `<div class="job-row">...</div>` chunks. <br /><br />
          <img src="{{ site.baseurl }}/assets/image/web_div.png" style="width:100%"> <br /> 
        
        * **For each chunk (or each position), we have following basic features.**
           * Job Title, Job ID and Job Link <br />
          <img src="{{ site.baseurl }}/assets/image/web_title.png" style="width:100%"> <br />
            * Job Company <br />
          <img src="{{ site.baseurl }}/assets/image/web_company.png" style="width:100%"> <br />
            * Location <br />
          <img src="{{ site.baseurl }}/assets/image/web_loc.png" style="width:100%"> 
        
        * **The code for picking out all the basic info for each job.**
          
          <pre><code class="python"># create an empty dataframe
          job_df_careerbuilder = pandas.DataFrame()
            
          ########################################
          ##### Loop for all the total pages #####
          ########################################
          for i in range(1, num_pages_careerbuilder+1):
            # generate the URL for each page
            url = ''.join([url_careerbuilder,'?page_number=', str(i)])
            print(url)

            # get the HTML code from the URL
            rawcode = requests.get(url)
            soup = bs4.BeautifulSoup(rawcode.text, "lxml")

            # pick out all the "div" with "class="job-row"
            divs = soup.findAll("div")
            job_divs = [jp for jp in divs if not jp.get('class') is None
                            and 'job-row' in jp.get('class')]
              
            # loop for each div chunk
            for job in job_divs:
              try:
                # job id
                id = job.find('h2',{'class' :'job-title'}).find('a').attrs['data-job-did']
                # job link related to job id
                link = BASE_URL_careerbuilder + '/job/' + id
                # job title
                title = job.find('h2', {'class' : 'job-title'}).text.strip()
                # job company
                company = job.find('div', {'class' : 'columns large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
                # job location
                location = job.find('div', {'class' : 'columns end large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
              except:
                continue

              job_df_careerbuilder = job_df_careerbuilder.append({'job_title': title,
                                'job_id': id,
                                'job_company': company,
                                'from':'Careerbuilder',
                                'job_location':location,            
                               'job_link':link},ignore_index=True)</code></pre>
          
          <pre><code class="language-text" data-lang="text">http://www.careerbuilder.com/jobs-data-scientist-in-New-York,NY?page_number=1
          http://www.careerbuilder.com/jobs-data-scientist-in-New-York,NY?page_number=2</code></pre>
        * **Save the dataframe and print it**
          <pre><code class="python"># reorder the columns of dataframe
          cols=['from','job_id','job_title','job_company','job_location','job_link']
          job_df_careerbuilder = job_df_careerbuilder[cols] 
            
          # delete the duplicated jobs using job link
          job_df_careerbuilder = job_df_careerbuilder.drop_duplicates(['job_link'], keep='first')
            
          # print the dimenstion of the dataframe
          print(job_df_careerbuilder.shape)</code></pre>
          `(29, 6)`
          <br /><br />
          <img src="{{ site.baseurl }}/assets/image/web_jobs2.png" style="width:100%"/>
          
* **<font size="4">Only Part I is done!</font>** <br />
  Yes, the dataframe we have now is not complete yet. Now, we only list out all the searching results in a nice `.csv` but this is not enough for helping us  efficiently find jobs. So, for the next post I will talk about how to scrape all the URLs in our `.csv` file and then to see if they have any contents or words we are interested in. The next post will be, **"How Web Scraping eases my job searching pain? - Part II : Scrape contents from a list of URLs"**. Again, all the example python code can be found in my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example). 
  
*** 
        
        
  
