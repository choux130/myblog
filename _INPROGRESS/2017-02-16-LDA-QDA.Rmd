---
layout: post
title: "Linear and Quadratic Discriminant Analysis (LDA and QDA)"
date: 2017-02-16
author: Yin-Ting
categories: [Methodology, R]
tags: [Classification, Supervised]
cover:  "/assets/image/ldaqda.png"
---
### Overview

*** 

### Details
* **<font size="4">Name</font>** <br />
  Linear Discriminant Analysis (LDA) <br />
  Quadratic Discriminant Analysis (QDA)

* **<font size="4">Data Type</font>** <br />
  * **Reponse Variable**: Categorical <br />
  * **Explanatory Variable**: Numeric 

* **<font size="4">Assumptions</font>** <br />
  Suppose $$X=(X_1, X_2,..., X_p) \in M_{n \times p}$$ and $$Y= \{1,2,...,K \}$$. 
  * **LDA** <br />
    $$P(X=x|Y=k)=N(\mu_k, \Sigma), \quad \mu_k\in M_{p \times 1}, \quad \Sigma \in M_{p \times p}$$ <br />
    Given a class, the density of $X$ is a Multivariate Normal Distribution with mean $\mu_k$ and covariance matrix $\Sigma$. The covariance matrix are the same in every class. 
  * **QDA** <br />
    $$P(X=x|Y=k)=N(\mu_k, \Sigma_k), \quad \mu_k\in M_{p \times 1}, \quad \Sigma_k \in M_{p \times p}$$ <br />
    The covariance matrix can not be the same in every class. 
  
* **<font size="4">Algorithm</font>** <br />
  Define a classifier function $$C \colon \mathbb{R}^p \rightarrow \{ 1, 2,..., K \}$$.
  * **LDA** <br />
  $$
  \begin{align}
  C(x) &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(Y=k|X=x) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(Y=k, X=x) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(X=x|Y=k)P(Y=k) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}\log N(\mu_k, \Sigma) +logP(Y=k) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}\log\frac{1}{(2\pi)^2|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(X-\mu_k)^T\Sigma^{-1}(X-\mu_k))+\log P(Y=k)\\
       &=\underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}X^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\log P(Y=k) \\
 \widehat{P}(Y&=k) = \frac{n_k}{n} \\
  \widehat{\mu_k} &= \frac{1}{n_k}\sum_{i=1}^{n_k}x_{ki} \\
  \hat{\Sigma} &= 
  \begin{pmatrix}
 &\widehat{Var}(X_1) &\widehat{Cov}(X_1,X_2)   &\cdots \\ 
 &\cdots   &\widehat{Var}(X_2)  &\cdots \\ 
 &\cdots  &\cdots  &\cdots \\ 
 &\cdots  &\cdots &\widehat{Var}(X_p)
  \end{pmatrix}
  \end{align} 
  $$
  
  * **QDA** <br />
  $$
  \begin{align}
  C(x) &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(Y=k|X=x) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(X=x|Y=k)P(Y=k) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}\log N(\mu_k, \Sigma_k) +logP(Y=k) \\
       &= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}\log\frac{1}{(2\pi)^2|\Sigma_k|^\frac{1}{2}}exp(-\frac{1}{2}(X-\mu_k)^T\Sigma_k^{-1}(X-\mu_k))+\log P(Y=k)\\
       &=\underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}-\frac{1}{2}X^T\Sigma_kX+X^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k-\frac{1}{2}\log|\Sigma_k|+\log P(Y=k) \\
  \widehat{P}(Y&=k) = \frac{n_k}{n} \\
  \widehat{\mu_k} &= \frac{1}{n_k}\sum_{i=1}^{n_k}x_{ki} \\
  \hat{\Sigma_k} &= 
  \begin{pmatrix}
 &\widehat{Var}(X_{k1}) &\widehat{Cov}(X_{k1},X_{k2})   &\cdots \\ 
 &\cdots  &\widehat{Var}(X_{k2})  &\cdots \\ 
 &\cdots  &\cdots  &\cdots \\ 
 &\cdots  &\cdots &\widehat{Var}(X_{kp})
  \end{pmatrix}
  \end{align}
  $$
  
The below plot shows the result of using LDA and QDA on the same data. (from P.111 in [The Elements of Statistical Learning:Data Mining, Inference, and Prediction)](https://statweb.stanford.edu/~tibs/ElemStatLearn/))
<img src="{{ site.baseurl }}/assets/image/ldaqda.png" style="width:500px;height:350px;" />
  
  
  
  
* **<font size="4">Strengths and Weaknesses</font>** <br /> 
  * **Strengths**: <br />
    1. Even the assumption is violoated, it still can give us a good prediction. 
  * **Weaknesses**: <br />
    1. It's hard to meet the assumption. 
    2. It only can be used in the numeric predictors. 
    3. It's sensitive to outliers. 
  
* **<font size="4">A Simple Example</font>** <br />

* **<font size="4">R code (e1071::naiveBayes())</font>** <br />

* **<font size="4">Further topics</font>** <br />
  * **Regularized Discriminant Analysis** 
  * **Reduced-Rank Linear Discriminant Analysis**
    To have more precise prediction, we can transforming the data by maximize the between-class covariance compared to the within-class covariance. 
<img src="{{ site.baseurl }}/assets/image/reduced-rank.png"  
style="width:500px;"/>